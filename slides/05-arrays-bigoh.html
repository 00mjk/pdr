<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>CS 2150: 05-arrays-bigoh slide set</title>
    <meta name="description" content="A set of slides for a course on Program and Data Representation">
    <meta name="author" content="Aaron Bloomfield">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="reveal.js/css/reveal.min.css">
    <link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
    <link rel="stylesheet" href="css/pdr.css">
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css"><!-- For syntax highlighting -->
    <script><!-- If the query includes 'print-pdf', use the PDF print sheet -->
      document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
    <script type="text/javascript" src="js/dhtmlwindow.js"></script>
    <script type="text/javascript" src="js/canvas.js"></script>
    <link rel="stylesheet" href="css/dhtmlwindow.css" type="text/css">
  </head>

  <body onload="canvasinit()">
    <div id="dhtmlwindowholder"><span style="display:none"></span></div>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">

	<section data-markdown><script type="text/template">
# CS 2150
&nbsp;
### Program and Data Representation
### Spring 2014
&nbsp;
<center><small>[Aaron Bloomfield](http://www.cs.virginia.edu/~asb) / [aaron@virginia.edu](mailto:aaron@virginia.edu) / [@bloomfieldaaron](http://twitter.com/bloomfieldaaron)</small></center>
<center><small>Repository: [github.com/aaronbloomfield/pdr](http://github.com/aaronbloomfield/pdr) / [&uarr;](index.html) / <a href="05-arrays-bigoh.html?print-pdf"><img class="print" width="20" src="images/print-icon.png"></a></small></center>
&nbsp;  
&nbsp;
## Arrays & Big-Oh
	</script></section>

	<section data-markdown><script type="text/template">
# Contents
&nbsp;  
[Arrays in C/C++](#/arrays)  
[Orders of Growth](#/growth)  
[Determining Running Times](#/runningtimes)  
	</script></section>


	<section>

	  <section id="arrays" data-markdown><script type="text/template">
# Arrays in C/C++
	  </script></section>

	  <section data-markdown><script type="text/template">
## Primitive Arrays in C++
- Declaring (two ways):
  - Specify name, type, and size
```
int someInts[3];
```
  - Specify name, type, and initialization list
```
int someInts[3] = {4, 37, 18};
```
- Either way, this creates 3 ints:
```
someInts[0], someInts[1], someInts[2]
```
	  </script></section>

	  <section data-markdown><script type="text/template">
## Things to remember about arrays in C++
1. No index checking!
   - Compiler and runtime system will not catch accesses out of array index bounds
   - Out of bounds access will cause unpredictable behavior
2. When passed as an argument to a function
   - Function does not know the size of the array
   - Need to pass another argument containing the size
3. Cannot be copied with =
   - Must be copied element by element
	  </script></section>

	  <section data-markdown><script type="text/template">
## C++ Implementation of Arrays
- Array names are pointers to beginning of array
```
int someInts[3];
```
  - size is only used by compiler to set aside memory for the array
  - `someInts` is set to `&someInts[0]`
  - Also called the base address
  - Local constant -- Cannot be changed
- Note that the previous declaration is almost the same as `int* someInts = new int[3];`
	  </script></section>

	  <section data-markdown><script type="text/template">
## int someInts[3];
(diagram)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Where is someInt[i] located in memory?
- Remember
  - `someInts` is a pointer to address of `someInts[0]` (i.e., `&someInt[0]`)
  - Array elements are laid out sequentially in contiguous memory addresses

```
&someInts[i] = {addr of someInts} + (sizeof(int) * i)
```
	  </script></section>

	  <section data-markdown><script type="text/template">
## Operations on Arrays
- `someInts = someOtherArray;` is  illegal
  - Array base addresses are considered constant (cannot be reassigned)
  - This is one example of when pointers and arrays are NOT the same
- Is `someInts == someOtherArray` allowed?
  - Yes, but will only evaluate to true if someInts and someOtherArray point at the same memory location
	  </script></section>

	  <section data-markdown><script type="text/template">
## Function Calls and Arrays
(no external source code)
```
someFunc(int arrayOfInts[]) { /* ... */ }

// ...

int main() {
    int someInts[3];
    someFunc(someInts); // no brackets
}
```
- `someInts` is a memory address
- Size of `someInts[]` unknown
- Is this pass by reference?  Pass by value?
	  </script></section>

	  <section data-markdown><script type="text/template">
## Multidimensional Arrays
(no external source code)
```
int m[2][3]; //2 rows, 3 columns
int m[2][3] =
    { {1,2,3}, {4,5,6} };
    // initialize row 0: 1,2,3
    // initialize row 1: 4,5,6
```
	  </script></section>

	  <section data-markdown><script type="text/template">
## C++ Implementation
(diagram)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Command-line parameters
- You can define the main() method as either:
  - `int main()`
  - `int main (int argc, char **argv)`
    - Or: `int main (int argc, char* argv[])`
- The second gives us two parameters:
  - The number of command line parameters
    - Note that the program executable is one of them (`argv[0]`)
  - An array of C-style strings (`char*`) containing those parameters
	  </script></section>

	</section>


	<section>

	  <section id="growth" data-markdown><script type="text/template">
# Orders of Growth
	  </script></section>

	  <section data-markdown><script type="text/template">
Orders of growth
Orders of Growth
Algorithm Analysis
Orders of growth classes
Running Time Calculations
Classifying functions by their
Asymptotic Growth Rates
Asymptotic growth rate, asymptotic order, or 
order of functions 
Comparing and classifying functions that ignores constant factors and small inputs. 

The sets big oh O(g), big theta (g), big omega (g)
Why do we care?
Some data structures are faster than others
Each data structure has some operations that are fast, and some that are slow
We need a way to compare them
This allows us to
Better choose the data structures that we will use
Better design additional data structures
Input sizes
Your algorithm does not matter if you have 10 elements
A shuffle sort will work just fine

Consider big input sizes:
UVa's e-mail has ~100,000 e-mail addresses
Mapquest (for driving routes) has ~20 million intersections
Google's index is 24 billion items

Even for small input sizes...
This is from the first lecture set
All times are in ms (1/1000th of a second)
The Sets  O(g), (g), (g)
Let g and f  be a functions from the nonnegative integers into the positive real numbers
For some real constant c > 0 and 
some nonnegative integer constant N0
O(g) is the set of functions f, such that 
f(N)  c g(N) for all N  n0
(g) is the set of functions f, such that 
f(N)  c g(N) for all N  n0
(g) = O(g)  (g)
(g) is the asymptotic order of g or the order of g
f (g) read as "f is asymptotic order g" or "f is order g"
Asymptotic Bounds
The Sets big oh O(g), big theta (g), big omega (g) -- remember these meanings:

O(g): functions that grow no faster than g,
or asymptotic upper bound

(g): functions that grow at least as fast as g,
or asymptotic lower bound

(g): functions that grow at the same rate as g,
or asymptotic tight bound
 O Examples
Given f  O (h) and g  O (h) which of these are true


For all positive integers m, f (m) < g (m).
For some positive integer m, f (m) < g (m).
For some positive integer m0, and all positive integers m > m0,  f (m) < g (m).
a and b
b and c
a and c
1 is false: 
Prove by Counter-Example
For all positive integers m, f (m) < g (m).
2 is true: Intuition
If f  O (h) and g  O (h) then, for some positive integer m, f (m) < g (m).

g must grow faster than h, otherwise g would be in O(h).
f must grow no faster than h, since 
f  O (h)
So, if g grows faster than h, but f grows as slow or slower than h, eventually, 
g(n) > f (n) so for some m, f (m) < g (m).

Given f  O (h) and g  O (h) which of these are true


For all positive integers m, f (m) < g (m).
For some positive integer m, f (m) < g (m).
For some positive integer m0, and all positive integers m > m0,  f (m) < g (m).
a and b
b and c
a and c
Lower Bound:  (Omega)
	f(n) is  (g (n)) means:
		There are positive constants c and n0 such that 	
			f (n)  cg(n) 
		for all n  n0 

Theta ("Order of")
Intuition: the set (f ) is the set of functions that grow as fast as f
Definition: f (n)   (g (n)) if and only if both: 
1. f (n)  O (g (n))
and 2. f (n)   (g (n))
Note: we do not have to pick the same c and n0 values for 1 and 2
When we say, "f is order g" that means 
f (n)   (g (n))
Tight Bound Theta ()
End of lecture on Wed, Sep 25
Little-oh
Let g(x)  o(f(x))
Any function that is o(f) is also O(f)
So both act as an upper bound
But a O(f) bound can also be (f) and thus (f)
Meaning the bound can be tight
Little-oh means the bound can not be tight
Thus, it's O(f) but not (f)
Or, that g(x) will ALWAYS be lower than f(x)
For some constants c and n0
Rarely used in computer science...
Little-omega
Analogous to little-oh
It's a lower bound that cannot be a tight lower bound
If something is (f), then it's (f), but not O(f) (or o(f) or (f))
Rarely used in computer science...
Another Way to Define Order Classes 
Comparing f(n) and g(n) as n approaches infinity, 
If

< , including the case in which the limit is 0 then 
f O(g)
> 0, including the case in which the limit is  then 
f  (g)
= c and 0 < c <  then f  (g)
= 0  then f  o(g)   //read as "little oh of g"
=   then f  (g)  //read as "little omega of g"
A note about notation
Technically, f(x) = O(h(x)) implies an equality
This is really an abuse of notation, but we can deal with it
It should be f(x)  O(h(x))
Some Properties of O(g), (g), (g)
Transitive: If f O(g) and g O(h), then f O(h)
O is transitive. Also , , o,  are transitive.
Reflexive: f  (f)
As is big-Oh and big-omega
Symmetric: If f  (g), then g  (f)
Big-Oh is not symmetric!  (neither is , o, or )
 defines an equivalence relation on the functions.
Each set (f) is an equivalence class (complexity class).
f O(g)  g  (f)
O(f + g) = O(max(f, g)) 
similar equations hold for  and 
Classification of functions
(1) denotes the set of functions bounded by a constant (for large n)
f  (n), f is linear
f  (n log n), f is log-linear
f  (n2), f is quadratic
f  (n3), f is cubic
f  (2n), f is exponential

nk  o(cn) for any k > 0 and any c > 1
powers of n grow more slowly than any exponential function cn
A note about logs
The difference between log10x and log2x is always a constant
Specifically, about 3.322
Since we don't care about constants in these analyses, we'll ignore the log base
Most things in CS are log base 2 anyway...
Typical Growth Rates
Does Order Class Matter?
No, not for small inputs
Yes, for many real problems
Practical Complexity
Practical Complexity
Practical Complexity
Practical Complexity
Practical Complexity
General Rules for Running Time Calculations (Big-Oh)

For loops
At most the running time of the statements inside the for loop
Nested loops
Analyze from inside to out.
Runtime of the statement * product of the sizes of the loops
Consecutive statements
Additive
if/else
Time for the test + longer of the runtimes
Determining Running Times
	  </script></section>

	</section>


	<section>

	  <section id="runningtimes" data-markdown><script type="text/template">
# Determining Running Times
	  </script></section>

	  <section data-markdown><script type="text/template">
## How to tell the running time...
- You need to imagine how long the algorithm would run given varying input sizes
  - We will assume our input size is *n*
- You should ask yourself...
  - How long will this algorithm take when *n* = 1?
  - When *n* = 100?
  - When *n* = 1,000,000,000?
	  </script></section>

	  <section data-markdown><script type="text/template">
## Running time: constant
- Constant (&Theta;(1)) time means that the running time does NOT depend on the size of the input
- Examples:
  - Getting a vector's size
  - Linked list insert or delete (at the known end)
  - Finding the kth element in an array or vector
	  </script></section>

	  <section data-markdown><script type="text/template">
## Running time: logarithmic
- Logarithmic (&Theta;(log *n*)) running time is when the search space is cut in half in each iteration
- Examples:
  - Binary search in a sorted list
  - Search through a balanced tree
  - Insert into a balanced tree
	  </script></section>

	  <section data-markdown><script type="text/template">
## Running time: linear
- Linear (&Theta;(*n*)) running time means that we have to process each element, and there is one step (or a constant number of steps) for each one
- Examples:
  - Printing (or otherwise iterating through) a list
  - Finding an element in an unsorted array
  - Finding an element in a sorted or unsorted linked list
  - Doubling the size of a vector's underlying array
	  </script></section>

	  <section data-markdown><script type="text/template">
## Running time: Log-linear
- Log-linear (&Theta;(*n* log *n*)) typically occurs when you are going to take a linear number of steps, but takes log *n* time
  - (or log *n* steps, each one of which takes *n* time)
- Examples:
  - Fast sorts: merge sort and heap sort
  - Quicksort, on a good day
  - Inserting n elements into a data structure where each insert takes log *n* time
	  </script></section>

	  <section data-markdown><script type="text/template">
## Running time: quadratic
- A quadratic (&Theta;(*n*<sup>2</sup>)) running time occurs when, for each input, you have to search through the entire input again
  - (among other ways it can occur)
- Examples:
  - Slow sorts: insertion sort, bubble sort, selection sort
  - Quicksort, on a bad day
  - Some graph algorithms we'll see later
  - Nested for loops (where each goes 1 to *n*)
	  </script></section>

	  <section data-markdown><script type="text/template">
## Running time: exponential
- An exponential (&Theta;(2<sup>*n*</sup>)) running time typically means you are going to try every possibility, and there are 2<sup>*n*</sup> of them
- Examples (we have not seen any yet)
  - Trying to crack a binary combination lock by trying every single possibility
  - Traveling salesperson problem (we'll see that later this semester)
	  </script></section>

	</section>


      </div>

    </div>

    <div id="calibratediv" style="display:none">
      <div id="calibratecanvasdiv">
        <canvas id="calibratecanvas" width="300" height="300">Your browser does not support the canvas tag</canvas>
      </div>
      <p style="text-align:center">Click the center of the target<br><a href="#" onClick="calibratewin.close(); return false">Close window</a></p>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.min.js"></script>
    <script src="js/settings.js"></script>

  </body>
</html>
